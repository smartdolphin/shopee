{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "* Image: nfnet-l0 + Efficientnet-b5 (Arcface Module)\n",
    "* Text : TfidfVectorizer + Sbert (Arcface Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pytorch-image-models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "import random \n",
    "import os \n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, AdaptiveAvgPool2d, Linear\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F \n",
    "from functools import reduce\n",
    "\n",
    "import transformers\n",
    "\n",
    "import gc\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Image\n",
    "    img_size = 512\n",
    "    batch_size = 16\n",
    "    seed = 42\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes1 = 11014\n",
    "    \n",
    "    model_name1 = 'eca_nfnet_l0'\n",
    "    model_path1 = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n",
    "    model_name2 = 'tf_efficientnet_b5'\n",
    "    model_path2 = '../input/shopee-hash-model-eff-b5/arcface_512x512_tf_efficientnet_b5(mish).pt'\n",
    "    model_name3 = 'efficientnet_b5'\n",
    "    model_path3 = '../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt'\n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5\n",
    "    \n",
    "    # Text\n",
    "    CV = False\n",
    "    num_workers=4\n",
    "    transformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n",
    "    text_model_path = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n",
    "    \n",
    "    model_params = {\n",
    "    'n_classes':11014,\n",
    "    'model_name':transformer_model,\n",
    "    'use_fc':False,\n",
    "    'fc_dim':512,\n",
    "    'dropout':0.3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(CFG.transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(is_train=False):\n",
    "    if is_train:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        if CFG.CV:\n",
    "            tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "            df['matches'] = df['label_group'].map(tmp)\n",
    "            df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "    return df, df_cu, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions1'], row['image_predictions3'], row['text_predictions'], row['text_predictions2']])\n",
    "    return ' '.join( np.unique(x))\n",
    "\n",
    "def combine_for_cv(row):\n",
    "    x = np.concatenate([row['image_predictions1'], row['image_predictions3'], row['text_predictions'], row['text_predictions2']])\n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings, threshold=0.0):\n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors=KNN, metric='cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in tqdm(range(embeddings.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
    "            A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeTextDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        text = row.title\n",
    "        \n",
    "        text = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = text['input_ids'][0]\n",
    "        attention_mask = text['attention_mask'][0]  \n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    \n",
    "class ShopeeTextNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 model_name='bert-base-uncased',\n",
    "                 use_fc=False,\n",
    "                 fc_dim=512,\n",
    "                 dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param n_classes:\n",
    "        :param model_name: name of model from pretrainedmodels\n",
    "            e.g. resnet50, resnext101_32x4d, pnasnet5large\n",
    "        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n",
    "        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n",
    "        \"\"\"\n",
    "        super(ShopeeTextNet, self).__init__()\n",
    "\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "        final_in_features = self.transformer.config.hidden_size\n",
    "        \n",
    "        self.use_fc = use_fc\n",
    "    \n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        feature = self.extract_feat(input_ids,attention_mask)\n",
    "        return F.normalize(feature)\n",
    "\n",
    "    def extract_feat(self, input_ids,attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        features = x[0]\n",
    "        features = features[:,0,:]\n",
    "\n",
    "        if self.use_fc:\n",
    "            features = self.dropout(features)\n",
    "            features = self.fc(features)\n",
    "            features = self.bn(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # cos(theta) & phi(theta)\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        # convert label to one-hot\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output\n",
    "\n",
    "class ShopeeModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.classes1,\n",
    "        model_name = CFG.model_name1,\n",
    "        fc_dim = 512,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(ShopeeModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == 'resnext50_32x4d':\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif 'efficientnet' in model_name:\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif 'tf_efficientnet_b5' in model_name:\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        elif 'eca_nfnet' in model_name:\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        final_in_features = fc_dim\n",
    "\n",
    "        self.final = ArcMarginProduct(\n",
    "            final_in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        #logits = self.final(feature,label)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish_func(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "  \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2) \n",
    "\n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        #grad_hv = 1./v  \n",
    "        #grad_vx = i.exp()\n",
    "        \n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
    "        \n",
    "        return grad_output * grad_f \n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "\n",
    "\n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    \n",
    "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
    "\n",
    "        if type(module) == existing_layer:\n",
    "            layer_old = module\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths, model_name=CFG.model_name1, model_path=CFG.model_path1, n_classes=CFG.classes1):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeModel(model_name=model_name, n_classes=n_classes)\n",
    "    model.eval()\n",
    "    \n",
    "    if model_name == 'eca_nfnet_l0' or model_name == 'tf_efficientnet_b5':\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(CFG.device)\n",
    "    \n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_predictions(df, max_features = 25_000, th=0.75):\n",
    "    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    preds = []\n",
    "    CHUNK = 1024*4\n",
    "\n",
    "    print('Finding similar titles...')\n",
    "    CTS = len(df)//CHUNK\n",
    "    if len(df)%CHUNK!=0: CTS += 1\n",
    "    for j in range( CTS ):\n",
    "\n",
    "        a = j*CHUNK\n",
    "        b = (j+1)*CHUNK\n",
    "        b = min(b,len(df))\n",
    "        print('chunk',a,'to',b)\n",
    "\n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
    "\n",
    "        for k in range(b-a):\n",
    "            IDX = cupy.where(cts[k,] > th)[0]\n",
    "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "            preds.append(o)\n",
    "    \n",
    "    del model, text_embeddings\n",
    "    gc.collect()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(df):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeTextNet(**CFG.model_params)\n",
    "    model.eval()\n",
    "    \n",
    "    model.load_state_dict(dict(list(torch.load(CFG.text_model_path).items())[:-1]))\n",
    "    model = model.to(CFG.device)\n",
    "\n",
    "    text_dataset = ShopeeTextDataset(df)\n",
    "    text_loader = torch.utils.data.DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=CFG.num_workers\n",
    "    )    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(text_loader): \n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            feat = model(input_ids, attention_mask)\n",
    "            text_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(text_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "   \n",
    "\n",
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len(np.intersect1d(row.target, row[col]))\n",
    "        return 2*n / (len(row.target) + len(row[col]))\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours_cos_sim(df,embeddings, threshold=0.6):\n",
    "    '''\n",
    "    When using cos_sim use normalized features else use normal features\n",
    "    '''\n",
    "    embeddings = cupy.array(embeddings)\n",
    "    \n",
    "    if CFG.CV:\n",
    "        thresholds = list(np.arange(0.5,0.7,0.05))\n",
    "\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            preds = []\n",
    "            CHUNK = 1024*4\n",
    "\n",
    "            print('Finding similar titles...for threshold :',threshold)\n",
    "            CTS = len(embeddings)//CHUNK\n",
    "            if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "            for j in range( CTS ):\n",
    "                a = j*CHUNK\n",
    "                b = (j+1)*CHUNK\n",
    "                b = min(b,len(embeddings))\n",
    "\n",
    "                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "                for k in range(b-a):\n",
    "                    IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                    o = ' '.join(o)\n",
    "                    preds.append(o)\n",
    "                    \n",
    "            df['pred_matches'] = preds\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "            \n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "            \n",
    "    else:\n",
    "        preds = []\n",
    "        CHUNK = 1024*4\n",
    "\n",
    "        print('Finding similar texts...for threshold :',threshold)\n",
    "        CTS = len(embeddings)//CHUNK\n",
    "        if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "        for j in range( CTS ):\n",
    "            a = j*CHUNK\n",
    "            b = (j+1)*CHUNK\n",
    "            b = min(b,len(embeddings))\n",
    "            print('chunk',a,'to',b)\n",
    "\n",
    "            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "                    \n",
    "    return df, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(*args):\n",
    "    return reduce(np.intersect1d, args)\n",
    "\n",
    "def concat(*args):\n",
    "    return np.unique(np.concatenate(args))\n",
    "\n",
    "def higher(row):\n",
    "    res = {}\n",
    "    context = np.concatenate([row['img1'], row['img3'], row['text'], row['text2']])\n",
    "    keys = np.unique(context)\n",
    "    for k in keys: \n",
    "        res[k] = np.count_nonzero(context == k)\n",
    "    output_dict = dict(filter(lambda item: item[1] >= 2, res.items()))\n",
    "    \n",
    "    return ' '.join((list(output_dict.keys())))\n",
    "\n",
    "def count(row):\n",
    "    res = {}\n",
    "    context = np.concatenate([row['img1'], row['img3'], row['text'], row['text2']])\n",
    "    keys = np.unique(context)\n",
    "    for k in keys: \n",
    "        res[k] = np.count_nonzero(context == k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df,df_cu,image_paths = read_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l0 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths.values, model_name=CFG.model_name1, model_path=CFG.model_path1, n_classes=CFG.classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_embeddings2 = get_image_embeddings(image_paths.values, model_name=CFG.model_name2, model_path=CFG.model_path2, n_classes=CFG.classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for efficientnet_b5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_embeddings3 = get_image_embeddings(image_paths.values, model_name=CFG.model_name3, model_path=CFG.model_path3, n_classes=CFG.classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2924.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3233.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.33)\n",
    "#image_predictions2 = get_image_predictions(df, image_embeddings2, threshold = 0.33)\n",
    "image_predictions3 = get_image_predictions(df, image_embeddings3, threshold = 0.33)\n",
    "text_predictions = get_text_predictions(df, max_features = 25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 768)\n"
     ]
    }
   ],
   "source": [
    "text_embeddings = get_text_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar texts...for threshold : 0.6\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "df, text_predictions2 = get_neighbours_cos_sim(df, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matches'] = pd.DataFrame({'img1': image_predictions,\n",
    "                              'img3': image_predictions3,\n",
    "                              'text': text_predictions,\n",
    "                              'text2': text_predictions2}).apply(higher, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_predictions'] = text_predictions\n",
    "#df['image_predictions'] = image_predictions2\n",
    "#df['matches_CV'] = df.apply(combine_for_cv, axis=1)\n",
    "#tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "#df['target'] = df.label_group.map(tmp)\n",
    "#MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n",
    "#print('CV score =', MyCVScore.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* efficientnet-b5 + tfidf\n",
    "* CV score = 0.8983709554601149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ensemble (nfnet_l0 + efficientnet-b5 + tfidf)\n",
    "- CV score = 0.8969412521795675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ensemble (nfnet_l0 + tf_eff_b5 + tfidf) using Count >= 2\n",
    "- CV score = 0.897"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
